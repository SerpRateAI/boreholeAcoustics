{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROSES Array and Network Processing\n",
    "\n",
    "This is a companion data exercise for the ROSES 2020 class on Array and Network Methods.\n",
    "\n",
    "Stephen Arrowsmith (sarrowsmith@smu.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import roses_array as ra\n",
    "from obspy import read\n",
    "import utm\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Array Processing\n",
    "\n",
    "The array processing algorithms described below are applied to data from the 2018/12/18 Bering Sea bolide. It's one of the most energetic events detected with optical measurements: https://cneos.jpl.nasa.gov/fireballs/\n",
    "\n",
    "The NASA estimated yield is 173 kT. For reference, the bomb dropped over Hiroshima was about 15 kT, and the IMS infrasound network is designed to detect events greater than 1 kT.\n",
    "\n",
    "### Reading infrasound data\n",
    "\n",
    "We're going to read data from the IMS infrasound array called I53US, which is in Fairbanks, AK. The array records a signal that has propagated in a waveguide between the ground and the top of the stratosphere. We'll apply a Cosine taper and bandpass filter it from 0.1 - 5 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = read('I53US.pickle')\n",
    "st.taper(type='cosine', max_percentage=0.05, max_length=60)\n",
    "st.filter('bandpass', freqmin=0.1, freqmax=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll plot the data using the built-in ObsPy Stream plot method. Try zooming in on the signal and zooming in on the pre-event noise. Does it look like the signal is coherent across the array? What about the noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "\n",
    "fig = st.plot(handle=True, method='full', type='relative', size=(600, 600))\n",
    "#fig.gca().set_xlim((1000,1800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the array coordinates\n",
    "\n",
    "I have this bad habit of sticking the sensor metadata in the ObsPy Trace objects under the sac attribute. You can access it using the handy get_array_coords method I provided for you. We can use this to view the array coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, stn = ra.get_array_coords(st, 'I53H6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "\n",
    "plt.plot(X[:,0], X[:,1], 'o')\n",
    "for i in range(0, len(stn)):\n",
    "    plt.text(X[i,0], X[i,1], stn[i])\n",
    "plt.xlabel('Distance (km)')\n",
    "plt.ylabel('Distance (km)')\n",
    "plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What frequency range is this array well suited for?\n",
    "\n",
    "(the average speed of sound in air is ~340 m/s, although Fairbanks is probably pretty cold...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting slowness-plane power\n",
    "\n",
    "This figure is conventionally called an FK plot, as it is normally computed in the frequency domain using the Frequency-Wavenumber algorithm. We're going to define a time window, and use some code I completely ripped out of ObsPy in order to plot an FK image (why doesn't ObsPy provide a handle for this?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a start time to compute an FK plot for:\n",
    "time_start = st[0].stats.starttime+1200\n",
    "time_end = st[0].stats.starttime+1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "\n",
    "baz, dist = ra.gc_backzimuth(st, 172.4, 56.9)\n",
    "print('GC backazimuth =', baz)\n",
    "fk = ra.plotFK(st, time_start, time_end, \n",
    "                    0.5, 5, \n",
    "                    sll_x=-3.6, slm_x=3.6, sll_y=-3.6, slm_y=3.6, \n",
    "                    sl_s=3.6/100, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing a beam\n",
    "\n",
    "Now that we've estimated a slowness vector that corresponds to the direction-of-arrival (DOA), let's compute the beam for that DOA. To do this, we first compute the time shifts for the backazimuth and phase velocity, then convert these to integer shifts, and finally compute and plot the beam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing array coordinates:\n",
    "X, stnm = ra.get_array_coords(st, 'I53H6')\n",
    "\n",
    "# Computing time-shifts for estimated signal DOA:\n",
    "t_shifts = ra.compute_tshifts(X, 269.33, 0.33)\n",
    "\n",
    "# Computing integer time shifts given the sampling interval:\n",
    "int_shifts = [int(x) for x in np.array(t_shifts)/st[0].stats.delta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "\n",
    "beam = ra.plot_beam(st, int_shifts, 'I53H6', return_beam=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing DOA with sliding-window array processing\n",
    "\n",
    "The next code block applies the least-squares method for estimating the DOA in a sliding-window. You can specify the start and end time to process, the length of the time window, and the overlap. The output is a list of times, phase velocities, and backazimuths. The subsequent plot shows the results with the beamformed waveform for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart = 500; tend = 2500; twin = 15; overlap = twin/4\n",
    "T, V, B = ra.sliding_time_array_lsq(st, X, tstart, tend, twin, overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "\n",
    "st = ra.add_beam_to_stream(st, beam, 'I53H6')\n",
    "ra.plot_sliding_window(st, T, B, V, v_max=2., twin_plot=[tstart,tend])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the estimates of direction-of-arrival prior to ~1250 s and after ~1750 s? What could be a disadvantage of using a time window that is (a) too short, and (b) too long? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing sliding-window FK\n",
    "\n",
    "The next two code blocks essentially implement the same procedure, but using the grid-search method and the built-in ObsPy Frequency-Wavenumber method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = st.select(station='I53*')    # Remove beam trace\n",
    "slid_fk = ra.sliding_time_array_fk(st, 500, 2500, win_len=15, win_frac=0.25, frqlow=0.1, frqhigh=5,\n",
    "                          sll_x=-3.6, slm_x=3.6, sll_y=-3.6, slm_y=3.6, sl_s=0.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "\n",
    "# Convert times to seconds after Stream start time:\n",
    "T = (slid_fk[:,0] - date2num(st[0].stats.starttime.datetime))*86400\n",
    "\n",
    "# Convert backazimuths to degrees from North:\n",
    "B = slid_fk[:,3] % 360.\n",
    "\n",
    "# Convert slowness to phase velocity:\n",
    "V = 1/slid_fk[:,4]\n",
    "\n",
    "# Semblance:\n",
    "S = slid_fk[:,1]\n",
    "\n",
    "#st = add_beam_to_stream(st, beam, 'I53H6')\n",
    "ra.plot_sliding_window(st, T, B, V, C=S, v_max=2., element='I53H1', twin_plot=[tstart,tend])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Local Similarity\n",
    "\n",
    "The next set of code blocks implement the Local Similarity method on data from the Oklahoma Wavefields experiment, described here: https://pubs.geoscienceworld.org/ssa/srl/article/89/5/1923/543213/A-Community-Experiment-to-Record-the-Full-Seismic\n",
    "\n",
    "Here are some challenge exercises:\n",
    "- Update the code to allow for a variable number of nearest neighbors (it is currently hardwired for 2)\n",
    "- Change the number of nearest neighbors to 5 and 10 and see how that affects the results. What are the fundamental limits of this techique in terms of distance and signal frequency?\n",
    "- Explore different filter bands (try 1/3 Octave bands)\n",
    "- Run on a 1-day time period, define a suitable trigger threshold, and make a catalog of detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = read('WAVEFIELDS.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting locations to UTM and finding nearest neighbor stations\n",
    "\n",
    "We're going to use the UTM package to convert locations to UTM coordinates (meters) and the sklearn package to find the nearest 2 neighbors for each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing X and Y locations (in meters) by converting from lat, lon to UTM:\n",
    "X = np.zeros((len(st), 2))\n",
    "for i in range(0, len(st)):\n",
    "    E, N, _, _ = utm.from_latlon(st[i].stats.sac.stla, st[i].stats.sac.stlo)\n",
    "    X[i,0] = E; X[i,1] = N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "\n",
    "plt.plot(X[:,0], X[:,1], '.')\n",
    "plt.xlabel('Easting (m)')\n",
    "plt.ylabel('Northing (m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint:\n",
    "Never re-invent the wheel *unless your goal is to gain understanding*:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/neighbors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing distances and indices to nearest neighboring stations:\n",
    "nbrs = NearestNeighbors(n_neighbors=3, algorithm='auto', metric='euclidean').fit(X)\n",
    "distances, ix = nbrs.kneighbors(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix[190:210,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances[190:210,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the correlation for a single station\n",
    "\n",
    "The next set of code blocks look at the correlation between the i'th station (where i can be adjusted) and the two other geographically closest stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking an example station (i'th station):\n",
    "i = 127\n",
    "print('Nearest stations =', ix[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "\n",
    "plt.plot(X[:,0], X[:,1], '.')\n",
    "plt.plot(X[ix[i,:],0], X[ix[i,:],1], 'r.')\n",
    "plt.xlabel('Easting (m)')\n",
    "plt.ylabel('Northing (m)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "\n",
    "# All the traces have the same start/end times and sampling rate:\n",
    "nsamp = len(st[0].data)\n",
    "t = np.linspace(-1., 10., nsamp)\n",
    "\n",
    "data = np.zeros((3, nsamp))\n",
    "k = 0; l = 0\n",
    "for j in ix[i,:]:\n",
    "    data[l,:] = st[j].data/np.max(np.abs(st[j].data))\n",
    "    plt.plot(t, k + data[l,:])\n",
    "    k = k + 1.5; l = l + 1\n",
    "plt.xlabel('Time (s) rel. to USGS origin time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy computes the cross-correlation defined by:\n",
    "\n",
    "$R(k) = \\sum_n x(n+k) \\overline{y(n)}$\n",
    "\n",
    "The normalized cross-correlation can be calculated from:\n",
    "\n",
    "$C(k) = \\frac{\\sum_n \\left( x(n+k) - \\mu_x \\right) \\overline{\\left( y(n) - \\mu_y \\right)}}{n \\sigma_x \\sigma_y}$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Cross-correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_xcorr(a, b, max_lag=10):\n",
    "    '''\n",
    "    Adds normalization to the NumPy cross correlation function and returns the maximum within\n",
    "    a time lag limit\n",
    "    \n",
    "    Returns the maximum correlation\n",
    "    '''\n",
    "    \n",
    "    a = (a - np.mean(a)) / (np.std(a) * len(a))\n",
    "    b = (b - np.mean(b)) / (np.std(b))\n",
    "    xcorr = np.correlate(a, b, 'full')\n",
    "    \n",
    "    i = len(a)-1                       # Zeroth lag\n",
    "    xcorr = xcorr[i-max_lag:i+max_lag]\n",
    "    return np.max(xcorr)\n",
    "\n",
    "# Defining the number of samples in the moving time window:\n",
    "delt = 1.\n",
    "nsamp_win = int(st[0].stats.sampling_rate * delt)\n",
    "\n",
    "# Defining maximum lag for computing the correlation in each time window:\n",
    "max_dist = np.max(distances) /1000.    # Maximum inter-station distance (km)\n",
    "v_min = 3.5                            # Slowest wave velocity (km/s)\n",
    "t_max = max_dist/v_min                 # Maximum inter-station travel time (s)\n",
    "i_max = int(t_max/st[0].stats.delta)   # Maximum necessary shift in samples\n",
    "\n",
    "# Computing the max correlation in a moving time window:\n",
    "C = []; L = []\n",
    "l = 0\n",
    "while l < nsamp-nsamp_win:\n",
    "    data_l = data[:,l:l+nsamp_win]\n",
    "    c1 = norm_xcorr(data_l[0,:], data_l[1,:], max_lag=i_max)\n",
    "    c2 = norm_xcorr(data_l[0,:], data_l[2,:], max_lag=i_max)\n",
    "    C.append((c1+c2)/2.)\n",
    "    L.append(l + int(nsamp_win/2))\n",
    "    l = l + int(nsamp_win/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "\n",
    "ax = plt.subplot(2,1,1)\n",
    "plt.plot(t[L], C)\n",
    "plt.ylabel('Local similarity')\n",
    "plt.subplot(2,1,2, sharex=ax)\n",
    "plt.plot(t, data[0,:])\n",
    "plt.plot(t, data[1,:])\n",
    "plt.plot(t, data[2,:])\n",
    "plt.xlabel('Time (s) rel. to USGS origin time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining a network similarity\n",
    "\n",
    "The next goal is to obtain a network similarity by stacking the individual similarity traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_stack = []\n",
    "\n",
    "for i in range(0, ix.shape[0]):\n",
    "    \n",
    "    # Extracting the data for the i'th station and neighbors:\n",
    "    data = np.zeros((3, nsamp))\n",
    "    k = 0; l = 0\n",
    "    for j in ix[i,:]:\n",
    "        data[l,:] = st[j].data/np.max(np.abs(st[j].data))\n",
    "        l = l + 1\n",
    "    \n",
    "    # Computing the max correlation in a moving time window:\n",
    "    C = []; L = []\n",
    "    l = 0\n",
    "    while l < nsamp-nsamp_win:\n",
    "        data_l = data[:,l:l+nsamp_win]\n",
    "        c1 = norm_xcorr(data_l[0,:], data_l[1,:], max_lag=i_max)\n",
    "        c2 = norm_xcorr(data_l[0,:], data_l[2,:], max_lag=i_max)\n",
    "        C.append((c1+c2)/2.)\n",
    "        L.append(l + int(nsamp_win/2))\n",
    "        l = l + int(nsamp_win/4)\n",
    "    \n",
    "    C_stack.append(C)\n",
    "\n",
    "C_stack = np.array(C_stack)\n",
    "C_stack2D = C_stack.copy()\n",
    "C_stack = C_stack.sum(axis=0)/ix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "\n",
    "ax = plt.subplot(2,1,1)\n",
    "plt.plot(t[L], C_stack)\n",
    "plt.ylabel('Local similarity')\n",
    "plt.subplot(2,1,2, sharex=ax)\n",
    "for i in range(0, ix.shape[0]):\n",
    "    plt.plot(t, st[i].data/np.max(np.abs(st[i].data)), 'k', linewidth=0.01)\n",
    "plt.xlabel('Time (s) rel. to USGS origin time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
